{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e79c0a4-c7e5-48f2-be2e-1f47f5e7a3ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM workspace.raw.protein \n",
    "WHERE blast_of_id IS NULL\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57ae2b3f-55d5-4c41-be12-ed3bcdf9781f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col, lag, collect_list, concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "fasta_df = spark.read.text(\"/Volumes/workspace/raw/input/700.fasta\")\n",
    "\n",
    "fasta_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b63f3b54-6b08-4371-9c15-44ddb41d09ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "fasta_df = spark.read.text(\"/Volumes/workspace/raw/input/700.fasta\")\n",
    "\n",
    "accession_df = fasta_df.filter(fasta_df.value.startswith(\">\"))\n",
    "accession_df = accession_df.withColumn(\"accession\", regexp_extract(col(\"value\"), \">(.+)\", 1))\n",
    "accession_numbers_df = accession_df.select(\"accession\")\n",
    "\n",
    "accession_numbers_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d10947-f65e-413c-8ae7-3886c59abe5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, regexp_extract, col, lag, collect_list, concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "fasta_df = spark.read.text(\"/Volumes/workspace/raw/input/700.fasta\")\n",
    "\n",
    "# Add a unique line number column using monotonically_increasing_id()\n",
    "fasta_df = fasta_df.withColumn(\"line_num\", monotonically_increasing_id())\n",
    "\n",
    "# Filter the accession lines and extract accession numbers\n",
    "accession_df = fasta_df.filter(fasta_df.value.startswith(\">\"))\n",
    "accession_df = accession_df.withColumn(\"accession\", regexp_extract(col(\"value\"), \">(.+)\", 1))\n",
    "\n",
    "# Window specification ordered by line number\n",
    "window_spec = Window.orderBy(\"line_num\")\n",
    "\n",
    "# Get line numbers of next accession lines for slicing sequences\n",
    "accession_line_df = accession_df.select(\"line_num\", \"accession\")\n",
    "accession_line_df = accession_line_df.withColumn(\"next_line_num\", lag(\"line_num\", -1).over(window_spec))\n",
    "\n",
    "# Join sequence lines between accession lines\n",
    "seq_df = fasta_df.join(\n",
    "    accession_line_df,\n",
    "    (fasta_df.line_num > accession_line_df.line_num) & \n",
    "    ((fasta_df.line_num < accession_line_df.next_line_num) | accession_line_df.next_line_num.isNull())\n",
    "    ).select(\"accession\", \"value\")\n",
    "\n",
    "# Group sequences by accession and concatenate sequence lines\n",
    "seq_by_accession = seq_df.groupBy(\"accession\").agg(concat_ws(\"\", collect_list(\"value\")).alias(\"sequence\"))\n",
    "\n",
    "# Show accession numbers and their sequences\n",
    "seq_by_accession.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ddecd7-b9ce-4894-ae63-366d30ae3489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, regexp_extract, col, lag, collect_list, concat_ws, current_timestamp, concat, lit\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "fasta_df = spark.read.text(\"/Volumes/workspace/raw/input/700.fasta\")\n",
    "\n",
    "# Add a unique line number\n",
    "fasta_df = fasta_df.withColumn(\"line_num\", monotonically_increasing_id())\n",
    "\n",
    "# Extract accession rows\n",
    "accession_df = fasta_df.filter(fasta_df.value.startswith(\">\"))\n",
    "accession_df = accession_df.withColumn(\"accession\", regexp_extract(col(\"value\"), \">(.+)\", 1))\n",
    "\n",
    "# Window spec for ordering by line_num\n",
    "window_spec = Window.orderBy(\"line_num\")\n",
    "\n",
    "# Get line number of next accession line for boundary\n",
    "accession_line_df = accession_df.select(\"line_num\", \"accession\")\n",
    "accession_line_df = accession_line_df.withColumn(\"next_line_num\", lag(\"line_num\", -1).over(window_spec))\n",
    "\n",
    "# Join sequences between acc lines\n",
    "seq_df = fasta_df.join(\n",
    "    accession_line_df,\n",
    "    (fasta_df.line_num >= accession_line_df.line_num) & \n",
    "    ((fasta_df.line_num < accession_line_df.next_line_num) | accession_line_df.next_line_num.isNull())\n",
    "    ).select(\"accession\", \"value\")\n",
    "\n",
    "# Group and concatenate sequence lines including header line\n",
    "seq_by_accession = seq_df.groupBy(\"accession\").agg(\n",
    "    concat_ws(\"\\n\", collect_list(\"value\")).alias(\"fasta_sequence\")\n",
    ")\n",
    "\n",
    "# Add columns for create and update timestamp, rename accession to id\n",
    "final_df = seq_by_accession.withColumn(\"record_create_ts\", current_timestamp()) \\\n",
    "                           .withColumn(\"record_update_ts\", current_timestamp()) \\\n",
    "                           .withColumnRenamed(\"accession\", \"id\") \\\n",
    "                           .select(\"record_create_ts\", \"record_update_ts\", \"id\", \"fasta_sequence\")\n",
    "\n",
    "final_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be59472a-8fbf-4b52-82d3-3da57f801c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.groupBy(\"id\").count().filter(col(\"count\") > 1).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd2a73e-cb3c-40b1-98f7-1be675daf9dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36a43ed5-d17c-43f6-b587-790c449e53d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM workspace.raw.protein "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79d2c0e5-b63d-46ed-b23c-659f798979e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_table = \"workspace.raw.protein\"\n",
    "\n",
    "# Append the final DataFrame to the UC table\n",
    "final_df.write.format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .saveAsTable(target_table)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7489266770759406,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "misc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
